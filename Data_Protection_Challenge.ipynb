{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Data Protection Challenges in Information Security and application of AI in mitigating the issue using machine learning\n",
        "# Importing necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report, confusion_matrix, cohen_kappa_score\n",
        "import warnings  # Warnings module is used to manage warnings in the code\n",
        "warnings.filterwarnings(\"ignore\")  # This line suppresses warnings to avoid clutter in the output\n",
        "df = pd.read_csv('/content/breach_report.csv')\n",
        "df"
      ],
      "metadata": {
        "id": "UDJutAwMzvOs",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MOTNZmw3PUtt"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t7n6-9QjJtmz"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Type of Breach'].value_counts()"
      ],
      "metadata": {
        "id": "ZnRxXWMh89NI",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic information about dataset\n",
        "print(\"\\nDataset Info:\")\n",
        "df.info()\n",
        "# Check for missing values\n",
        "print(\"\\nMissing values per column:\")\n",
        "print(df.isnull().sum())\n",
        "# Replace placeholder '\\N' with actual NaN values\n",
        "df.replace('\\\\N', np.nan, inplace=True)\n",
        "# Convert 'Breach Submission Date' to datetime format\n",
        "df['Breach Submission Date'] = pd.to_datetime(df['Breach Submission Date'], errors='coerce')\n",
        "# Convert 'Individuals Affected' to numeric\n",
        "df['Individuals Affected'] = pd.to_numeric(df['Individuals Affected'], errors='coerce')\n",
        "# Summary statistics\n",
        "print(\"\\nSummary statistics for numeric columns:\")\n",
        "print(df.describe())"
      ],
      "metadata": {
        "id": "4soMiSPRz6h0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check number of unique values in categorical columns\n",
        "print(\"\\nUnique values in categorical columns:\")\n",
        "categorical_columns = df.select_dtypes(include='object').columns\n",
        "for col in categorical_columns:\n",
        "    print(f\"{col}: {df[col].nunique()}\")\n"
      ],
      "metadata": {
        "id": "pgC_UYnCz6k1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Top 5 states with most breaches\n",
        "print(\"\\nTop 5 states with most breaches:\")\n",
        "print(df['State'].value_counts().head())\n",
        "# Top 5 types of breaches\n",
        "print(\"\\nMost common breach types:\")\n",
        "print(df['Type of Breach'].value_counts().head())"
      ],
      "metadata": {
        "id": "hsKbIq38z6oW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace placeholder '\\N' with NaN\n",
        "df.replace('\\\\N', np.nan, inplace=True)\n",
        "\n",
        "# Convert date and numeric columns\n",
        "df['Breach Submission Date'] = pd.to_datetime(df['Breach Submission Date'], errors='coerce')\n",
        "df['Individuals Affected'] = pd.to_numeric(df['Individuals Affected'], errors='coerce')\n",
        "\n",
        "# ----------- Missing Value Handling -----------\n",
        "\n",
        "# 1. 'State': fill with 'Unknown' as it is a location-based identifier\n",
        "df['State'].fillna('Unknown', inplace=True)\n",
        "\n",
        "# 2. 'Covered Entity Type': fill missing values with mode (most common type)\n",
        "df['Covered Entity Type'].fillna(df['Covered Entity Type'].mode()[0], inplace=True)\n",
        "\n",
        "# 3. 'Individuals Affected': fill with median (robust to outliers)\n",
        "df['Individuals Affected'].fillna(df['Individuals Affected'].median(), inplace=True)\n",
        "\n",
        "# 4. 'Type of Breach': fill with 'Unknown'\n",
        "df['Type of Breach'].fillna('Unknown', inplace=True)\n",
        "\n",
        "# 5. 'Location of Breached Info': fill with 'Unknown'\n",
        "df['Location of Breached Information'].fillna('Unknown', inplace=True)\n",
        "\n",
        "# 6. 'Web Description': drop column due to large number of missing values (not suitable for numeric analysis)\n",
        "df.drop(columns=['Web Description'], inplace=True)"
      ],
      "metadata": {
        "id": "XYFUOQSH09EQ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "TvfxHOCYMjL9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------- Graphs and Analysis -----------\n",
        "plt.figure(figsize=(8,6))\n",
        "# 1. Bar Chart - Top 10 Breach Types\n",
        "sns.countplot(data=df, x='Type of Breach', order=df['Type of Breach'].value_counts().head(10).index)\n",
        "plt.title(\"Top 10 Types of Data Breaches\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "REfz5OlU09HK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Pie Chart - Covered Entity Types\n",
        "plt.figure(figsize=(6, 6))\n",
        "df['Covered Entity Type'].value_counts().plot.pie(autopct='%1.1f%%', startangle=90, colors=sns.color_palette(\"pastel\"))\n",
        "plt.title(\"Distribution of Covered Entity Types\")\n",
        "plt.ylabel(\"\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HLWsioMF09KC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Histogram - Distribution of Individuals Affected\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.histplot(df['Individuals Affected'], bins=30, kde=True, color='orange')\n",
        "plt.title(\"Histogram: Individuals Affected by Breach\")\n",
        "plt.xlabel(\"Individuals Affected\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "zwW4Dn273xHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Bar Chart - Top 10 States with Breaches\n",
        "plt.figure(figsize=(8,5))\n",
        "df['State'].value_counts().head(10).plot(kind='bar', color='skyblue')\n",
        "plt.title(\"Top 10 States by Number of Breaches\")\n",
        "plt.xlabel(\"State\")\n",
        "plt.ylabel(\"Number of Breaches\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LVSfaM2T3xJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Scatter Plot - Submission Date vs. Individuals Affected\n",
        "plt.figure(figsize=(8,4))\n",
        "sns.scatterplot(x='Breach Submission Date', y='Individuals Affected', data=df, alpha=0.5)\n",
        "plt.title(\"Individuals Affected Over Time\")\n",
        "plt.xlabel(\"Breach Submission Date\")\n",
        "plt.ylabel(\"Individuals Affected\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ldkvmW7I3xMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Heatmap - Correlation Matrix (only numeric columns)\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(df.select_dtypes(include='number').corr(), annot=True, cmap='coolwarm')\n",
        "plt.title(\"Correlation Heatmap\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "w9j9E4zg37Wd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bar Chart - Business Associate involvement\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.countplot(data=df, x='Business Associate Present', palette='Set2')\n",
        "plt.title(\"Breaches Involving Business Associates\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-A5wpI8N37Zw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pie Chart - Location of Breach (Top 5)\n",
        "top_locations = df['Location of Breached Information'].value_counts().head(5)\n",
        "plt.figure(figsize=(6, 6))\n",
        "top_locations.plot.pie(autopct='%1.1f%%', startangle=140, colors=sns.color_palette('Set3'))\n",
        "plt.title(\"Top 5 Locations of Breached Information\")\n",
        "plt.ylabel(\"\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "28mMHfSl37ci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bar Chart - Top 5 Covered Entities with Most Affected Individuals (Summed)\n",
        "top_entities = df.groupby('Covered Entity Type')['Individuals Affected'].sum().sort_values(ascending=False).head()\n",
        "plt.figure(figsize=(8, 5))\n",
        "top_entities.plot(kind='bar', color='purple')\n",
        "plt.title(\"Top Entity Types by Total Individuals Affected\")\n",
        "plt.ylabel(\"Total Individuals Affected\")\n",
        "plt.xlabel(\"Covered Entity Type\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fC48IAi537fS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(columns=['Breach Submission Date'])"
      ],
      "metadata": {
        "id": "QLeKKT8G6ugW"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Top 5 breach types\n",
        "top_5_types = df['Type of Breach'].value_counts().nlargest(5).index.tolist()\n",
        "\n",
        "# Replace all other types with 'Other'\n",
        "df['Type of Breach'] = df['Type of Breach'].apply(lambda x: x if x in top_5_types else 'Other')\n",
        "# ----------------- Label Encoding Categorical Columns -----------------\n",
        "# Identify categorical columns\n",
        "categorical_cols = df.select_dtypes(include=\"object\").columns\n",
        "\n",
        "# Apply label encoding to each categorical column\n",
        "le_dict = {}  # Save label encoders in case we need them later (e.g., for inverse transform)\n",
        "for col in categorical_cols:\n",
        "    le = LabelEncoder()\n",
        "    df[col] = le.fit_transform(df[col])\n",
        "    le_dict[col] = le  # Store encoder for future use\n",
        "\n",
        "# Show label mapping for 'Type of Breach'\n",
        "type_of_breach_le = le_dict['Type of Breach']\n",
        "print(\"\\nLabel mapping for 'Type of Breach':\")\n",
        "for i, label in enumerate(type_of_breach_le.classes_):\n",
        "    print(f\"{i}: {label}\")"
      ],
      "metadata": {
        "id": "hQFb6wuM37iW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scale 'Individuals Affected' using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "df[\"Individuals Affected\"] = scaler.fit_transform(df[[\"Individuals Affected\"]])\n"
      ],
      "metadata": {
        "id": "u3HfeGnX3xPH"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------- Train-Test Split --------------------\n",
        "target = \"Type of Breach\"\n",
        "X = df.drop(columns=[target])\n",
        "y = df[target]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n"
      ],
      "metadata": {
        "id": "wr019wPs3xR4"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8vR62y876TEv"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------- Apply SMOTE Safely --------------------\n",
        "# Check class distribution\n",
        "class_counts = Counter(y_train)\n",
        "min_class_samples = min(class_counts.values())\n",
        "\n",
        "# Set k_neighbors safely\n",
        "k_neighbors = min(min_class_samples - 1, 5)\n",
        "sm = SMOTE(random_state=42, k_neighbors=k_neighbors)\n",
        "\n",
        "X_train_bal, y_train_bal = sm.fit_resample(X_train, y_train)"
      ],
      "metadata": {
        "id": "I3QbYNft51P5"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------- Output Class Distributions --------------------\n",
        "print(\"Original class distribution (before SMOTE):\")\n",
        "print(pd.Series(y_train).value_counts())\n",
        "\n",
        "print(\"\\nBalanced class distribution (after SMOTE):\")\n",
        "print(pd.Series(y_train_bal).value_counts())"
      ],
      "metadata": {
        "id": "6K87b5Ov51S6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dictionary to store the best models\n",
        "best_models = {}\n",
        "\n",
        "# Dictionary to store classification reports\n",
        "model_reports = {}\n",
        "\n",
        "# -------------------- 1. Logistic Regression --------------------\n",
        "print(\"🔍 Logistic Regression\")\n",
        "\n",
        "# Define hyperparameters to tune\n",
        "lr_params = {\n",
        "    'C': [0.01, 0.1, 1, 10],              # Regularization strength\n",
        "    'penalty': ['l2'],                    # Type of penalty\n",
        "    'solver': ['lbfgs', 'liblinear']      # Solvers compatible with l2\n",
        "}\n",
        "\n",
        "# Initialize the model\n",
        "lr = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# GridSearchCV for hyperparameter tuning\n",
        "lr_grid = GridSearchCV(lr, lr_params, cv=5, scoring='f1_weighted', n_jobs=-1)\n",
        "lr_grid.fit(X_train_bal, y_train_bal)  # Train the model on balanced training data\n",
        "\n",
        "# Save the best model\n",
        "best_models['Logistic Regression'] = lr_grid.best_estimator_\n",
        "\n",
        "# Predict on test set\n",
        "y_pred_lr = lr_grid.predict(X_test)\n",
        "\n",
        "# Print best parameters and evaluation metrics\n",
        "print(\"\\nBest Params:\", lr_grid.best_params_)\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_lr))\n",
        "print(\"Kappa Score:\", cohen_kappa_score(y_test, y_pred_lr))\n",
        "\n",
        "# Save classification report\n",
        "model_reports['Logistic Regression'] = classification_report(y_test, y_pred_lr, output_dict=True)\n",
        "\n",
        "# -------------------- 2. Decision Tree --------------------\n",
        "print(\"\\n🌳 Decision Tree\")\n",
        "\n",
        "# Define hyperparameter grid\n",
        "dt_params = {\n",
        "    'max_depth': [5, 10, 20, None],           # Tree depth\n",
        "    'min_samples_split': [2, 5, 10]           # Minimum samples to split a node\n",
        "}\n",
        "\n",
        "# Initialize model\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Grid search\n",
        "dt_grid = GridSearchCV(dt, dt_params, cv=5, scoring='f1_weighted', n_jobs=-1)\n",
        "dt_grid.fit(X_train_bal, y_train_bal)\n",
        "\n",
        "# Save best model\n",
        "best_models['Decision Tree'] = dt_grid.best_estimator_\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_dt = dt_grid.predict(X_test)\n",
        "\n",
        "print(\"\\nBest Params:\", dt_grid.best_params_)\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_dt))\n",
        "print(\"Kappa Score:\", cohen_kappa_score(y_test, y_pred_dt))\n",
        "\n",
        "model_reports['Decision Tree'] = classification_report(y_test, y_pred_dt, output_dict=True)\n",
        "\n",
        "# -------------------- 3. Random Forest --------------------\n",
        "print(\"\\n🌲 Random Forest\")\n",
        "\n",
        "# Hyperparameter grid\n",
        "rf_params = {\n",
        "    'n_estimators': [50, 100, 200],            # Number of trees\n",
        "    'max_depth': [10, 20, None],               # Max depth\n",
        "    'min_samples_split': [2, 5]                # Node split control\n",
        "}\n",
        "\n",
        "# Initialize model\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Grid search\n",
        "rf_grid = GridSearchCV(rf, rf_params, cv=5, scoring='f1_weighted', n_jobs=-1)\n",
        "rf_grid.fit(X_train_bal, y_train_bal)\n",
        "\n",
        "# Save best model\n",
        "best_models['Random Forest'] = rf_grid.best_estimator_\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_rf = rf_grid.predict(X_test)\n",
        "\n",
        "print(\"\\nBest Params:\", rf_grid.best_params_)\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_rf))\n",
        "print(\"Kappa Score:\", cohen_kappa_score(y_test, y_pred_rf))\n",
        "\n",
        "model_reports['Random Forest'] = classification_report(y_test, y_pred_rf, output_dict=True)\n",
        "\n",
        "# -------------------- 4. K-Nearest Neighbors --------------------\n",
        "print(\"\\n👥 K-Nearest Neighbors\")\n",
        "\n",
        "# Hyperparameter grid\n",
        "knn_params = {\n",
        "    'n_neighbors': [3, 5, 7],           # Number of neighbors\n",
        "    'weights': ['uniform', 'distance']  # Weighting method\n",
        "}\n",
        "\n",
        "# Initialize model\n",
        "knn = KNeighborsClassifier()\n",
        "\n",
        "# Grid search\n",
        "knn_grid = GridSearchCV(knn, knn_params, cv=5, scoring='f1_weighted', n_jobs=-1)\n",
        "knn_grid.fit(X_train_bal, y_train_bal)\n",
        "\n",
        "# Save best model\n",
        "best_models['KNN'] = knn_grid.best_estimator_\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_knn = knn_grid.predict(X_test)\n",
        "\n",
        "print(\"\\nBest Params:\", knn_grid.best_params_)\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_knn))\n",
        "print(\"Kappa Score:\", cohen_kappa_score(y_test, y_pred_knn))\n",
        "\n",
        "model_reports['KNN'] = classification_report(y_test, y_pred_knn, output_dict=True)\n",
        "\n",
        "# -------------------- Compare Confusion Matrices --------------------\n",
        "def plot_confusion_matrix(y_true, y_pred, title):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(8,4))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=type_of_breach_le.classes_, yticklabels=type_of_breach_le.classes_)\n",
        "    plt.title(f'Confusion Matrix - {title}')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot confusion matrices\n",
        "plot_confusion_matrix(y_test, y_pred_lr, \"Logistic Regression\")\n",
        "plot_confusion_matrix(y_test, y_pred_dt, \"Decision Tree\")\n",
        "plot_confusion_matrix(y_test, y_pred_rf, \"Random Forest\")\n",
        "plot_confusion_matrix(y_test, y_pred_knn, \"KNN\")"
      ],
      "metadata": {
        "id": "3LqhEtfl51Vt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "import os\n",
        "\n",
        "# Create a directory to save models if it doesn't exist\n",
        "os.makedirs(\"saved_models\", exist_ok=True)\n",
        "\n",
        "# Loop through the best models dictionary and save each model\n",
        "for name, model in best_models.items():\n",
        "    filename = f\"saved_models/{name.replace(' ', '_').lower()}_model.pkl\"\n",
        "    joblib.dump(model, filename)\n",
        "    print(f\"✅ Saved: {filename}\")"
      ],
      "metadata": {
        "id": "XjrbJiAJ51Yk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X1Et6-u709NG"
      },
      "execution_count": 24,
      "outputs": []
    }
  ]
}